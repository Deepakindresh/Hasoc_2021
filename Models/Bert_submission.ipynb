{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_submission.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Su45IUJrdX4u",
        "Qp_zCdofVU8E",
        "qxSSt7hmTP_k"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3IpYUh5ROzM"
      },
      "source": [
        "# Mount Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6dTF5JIRjli"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su45IUJrdX4u"
      },
      "source": [
        "# Importing Libraries and initializing stopwords and stemmer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-rDxARCdX45"
      },
      "source": [
        "!pip3 install texthero\n",
        "!pip3 install transformers\n",
        "!pip3 install tensorflow_addons\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ujYq76FI-3R"
      },
      "source": [
        "!pip install tweet-preprocessor\n",
        "\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmTaupdPJMl-"
      },
      "source": [
        "import re \n",
        "import nltk\n",
        "from wordcloud import WordCloud\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from textblob import TextBlob,Word\n",
        "from nltk.corpus import words\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "import texthero as hero\n",
        "import re\n",
        "from texthero import stopwords\n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score,f1_score, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x543u_5sJRbu"
      },
      "source": [
        "def lemma_per_pos(sent):\n",
        "    '''function to lemmatize according to part of speech tag'''\n",
        "    tweet_tokenizer=TweetTokenizer()\n",
        "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "    lemmatized_list = [lemmatizer.lemmatize(w) for w in  tweet_tokenizer.tokenize(sent)]\n",
        "    return \" \".join(lemmatized_list)\n",
        "\n",
        "def df_preprocessing(df,feature_col):\n",
        "    '''\n",
        "    Preprocessing of dataframe\n",
        "    '''\n",
        "    stop = set(stopwords.words('english'))\n",
        "    df[feature_col]= (df[feature_col].pipe(hero.lowercase).\n",
        "                      pipe(hero.remove_urls).\n",
        "                      pipe(hero.remove_digits).\n",
        "                      pipe(hero.remove_punctuation).\n",
        "                      pipe(hero.remove_html_tags) )\n",
        "    # lemmatization\n",
        "    df[feature_col]= [lemma_per_pos(sent) for sent in df[feature_col]]\n",
        "    #df[col_name]= hero.remove_stopwords(df[col_name],custom_stopwords)\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Llt35qZtVx0n"
      },
      "source": [
        "from transformers import AutoTokenizer,TFDistilBertModel, DistilBertConfig\n",
        "from transformers import TFAutoModel\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp_zCdofVU8E"
      },
      "source": [
        "# Reading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ql_BZvzaVYdM"
      },
      "source": [
        "hasoc_data = pd.read_csv(\"drive/MyDrive/Dataset/Hasoc2021_train.csv\") #Your respective address\n",
        "hasoc_data.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfv5ntOLMX0_"
      },
      "source": [
        "aakash_test = pd.read_csv(\"drive/MyDrive/Dataset/aakash_300.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZxttlPQMf-g"
      },
      "source": [
        "aakash_test.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrdUqYjAMtDA"
      },
      "source": [
        "aakash_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEVt58vTQ3KQ"
      },
      "source": [
        "test_hasoc = pd.read_csv(\"drive/MyDrive/Dataset/en_Hasoc2021_test_task1.csv\")\n",
        "test_hasoc.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcL3_A-fcKoB"
      },
      "source": [
        "hasoc_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdITNHuSMjxk"
      },
      "source": [
        "test_hasoc_2 = pd.read_csv(\"drive/MyDrive/Dataset/en_Hasoc2021_test_task2.csv\")\n",
        "test_hasoc_2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaRapolnN4Pw"
      },
      "source": [
        "#reading the data\n",
        "data= pd.read_csv('drive/MyDrive/Dataset/modified_hasoc_task21.csv')\n",
        "train_df = data\n",
        "# test_df = aakash_test\n",
        "#test_df= pd.read_csv('/kaggle/input/jigsaw-toxic-comment-classification-challenge/test.csv.zip')   TEST DATA TO BE PROVIDED\n",
        "print(train_df.columns,train_df.shape)\n",
        "target_col= train_df.columns[2:]\n",
        "feature_col= train_df.columns[1:2]\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btE66c78Ri5D"
      },
      "source": [
        "column_names = [\"id\",\"text\",\"hate\",\"offen\",\"prof\",\"none\"]\n",
        "test_df = pd.DataFrame(columns = column_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbW_bmxaR9I-"
      },
      "source": [
        "for i in range(len(aakash_test)):\n",
        "  id_temp = aakash_test['_id'].iloc[i]\n",
        "  text_temp = aakash_test['text'].iloc[i]\n",
        "  if aakash_test['task_2'].iloc[i] == \"NONE\":\n",
        "    test_df.loc[i] = [id_temp,text_temp,0,0,0,1]\n",
        "  elif aakash_test['task_2'].iloc[i] == \"PRFN\":\n",
        "    test_df.loc[i] = [id_temp,text_temp,0,0,1,0]\n",
        "  elif aakash_test['task_2'].iloc[i] == \"OFFN\":\n",
        "    test_df.loc[i] = [id_temp,text_temp,0,1,0,0]\n",
        "  elif aakash_test['task_2'].iloc[i] == \"HATE\":\n",
        "    test_df.loc[i] = [id_temp,text_temp,1,0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Niu23RS6ha"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxSSt7hmTP_k"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4XsYGROTSZ-"
      },
      "source": [
        " target_col,feature_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15j637nlK5Li"
      },
      "source": [
        "with tf.device('/GPU:0'): \n",
        " proc_train_df= df_preprocessing(train_df,feature_col[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vse9O-vXX440"
      },
      "source": [
        "proc_test_df = df_preprocessing(test_df,feature_col[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KHI7Jx9T4qL"
      },
      "source": [
        "#Creating tokenizer\n",
        "def create_tokenizer(pretrained_weights='distilbert-base-uncased'):\n",
        "  '''Function to create the tokenizer'''\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(pretrained_weights)\n",
        "  return tokenizer\n",
        "\n",
        "#Tokenization of the data\n",
        "def data_tokenization(dataset,feature_col,max_len,tokenizer):\n",
        "    '''dataset: Pandas dataframe with feature name is column name \n",
        "    Pretrained_weights: selected model \n",
        "    RETURN: [input_ids, attention_mask]'''\n",
        "\n",
        "    tokens = dataset[feature_col].apply(lambda x: tokenizer(x,return_tensors='tf', \n",
        "                                                            truncation=True,\n",
        "                                                            padding='max_length',\n",
        "                                                            max_length=max_len, \n",
        "                                                            add_special_tokens=True))\n",
        "    input_ids= []\n",
        "    attention_mask=[]\n",
        "    for item in tokens:\n",
        "        input_ids.append(item['input_ids'])\n",
        "        attention_mask.append(item['attention_mask'])\n",
        "    input_ids, attention_mask=np.squeeze(input_ids), np.squeeze(attention_mask)\n",
        "\n",
        "\n",
        "    return [input_ids,attention_mask]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhJAd1djU6aY"
      },
      "source": [
        "#Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CiaKJGWU88n"
      },
      "source": [
        "def bert_model(pretrained_weights,max_len,learning_rate):\n",
        "  '''BERT model creation with pretrained weights\n",
        "  INPUT:\n",
        "  pretrained_weights: Language model pretrained weights\n",
        "  max_len: input length '''\n",
        "  print('Model selected:', pretrained_weights)\n",
        "  bert=TFAutoModel.from_pretrained(pretrained_weights)\n",
        "  \n",
        "  # This is must if you would like to train the layers of language models too.\n",
        "  for layer in bert.layers:\n",
        "      layer.trainable = True\n",
        "\n",
        "  # # parameter declaration\n",
        "  # step = tf.Variable(0, trainable=False)\n",
        "  # schedule = tf.optimizers.schedules.PiecewiseConstantDecay([10000, 15000], [2e-0, 2e-1, 1e-2])\n",
        "  # # lr and wd can be a function or a tensor\n",
        "  # lr = learning_rate * schedule(step)\n",
        "  # wd = lambda:lr * schedule(step)\n",
        "  # optimizer = tfa.optimizers.AdamW(learning_rate=lr, weight_decay=wd)\n",
        "\n",
        "  optimizer= tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam')\n",
        "#   optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "  # declaring inputs, BERT take input_ids and attention_mask as input\n",
        "  input_ids= Input(shape=(max_len,),dtype=tf.int32,name='input_ids')\n",
        "  attention_mask=Input(shape=(max_len,),dtype=tf.int32,name='attention_mask')\n",
        "\n",
        "  bert= bert(input_ids,attention_mask=attention_mask)\n",
        "  x= bert[0][:,0,:]\n",
        "  x=tf.keras.layers.Dropout(0.12)(x)\n",
        "  # x= tf.keras.layers.Dense(128)(x)\n",
        "  x=tf.keras.layers.Dense(64)(x)\n",
        "  x=tf.keras.layers.Dense(32)(x)\n",
        "\n",
        "  output=tf.keras.layers.Dense(4,activation='sigmoid')(x)\n",
        "\n",
        "  model=Model(inputs=[input_ids,attention_mask],outputs=[output])\n",
        "  # compiling model \n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE,name='binary_crossentropy'),\n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lFaaJ01VIdC"
      },
      "source": [
        "pretrained_weights='bert-base-uncased'\n",
        "max_len=256\n",
        "epochs=15\n",
        "learning_rate=2e-5\n",
        "batch_size=4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A0lKezOZjba"
      },
      "source": [
        "tokenizer= create_tokenizer(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL-WzJP2Zr5h"
      },
      "source": [
        "x_train= data_tokenization(proc_train_df,feature_col[0],max_len,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qozXQ7m1sHQk"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdQJhutIZus-"
      },
      "source": [
        "y_train= proc_train_df[target_col].values\n",
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPhs_wfgsDzE"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1zXPHjtZxG8"
      },
      "source": [
        "bert=bert_model(pretrained_weights,max_len,learning_rate)\n",
        "bert.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9D6b83UWmoZ"
      },
      "source": [
        "with tf.device('/GPU:0'):\n",
        "    bert.fit(x_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aId9fg6CYSIE"
      },
      "source": [
        "#Aakash Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFDZDWxzaFMb"
      },
      "source": [
        "test_ids= test_df['id']\n",
        "x_test= data_tokenization(proc_test_df,feature_col[0],max_len,tokenizer)\n",
        "x_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQHj8LSrhVbU"
      },
      "source": [
        "test_df['id']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXFWXfllaOEF"
      },
      "source": [
        "preds= bert.predict(x_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmWazzyWbKsC"
      },
      "source": [
        "aakash_df= pd.DataFrame(preds, columns= target_col)\n",
        "aakash_df['id']=list(test_df['id'])\n",
        "aakash_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kylNcM1whFuF"
      },
      "source": [
        "aakash_df = aakash_df[['id']+target_col.tolist()]\n",
        "aakash_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPVvhv5zZMwL"
      },
      "source": [
        "column_names = [\"id\", \"label\"]\n",
        "aakash_result_df = pd.DataFrame(columns = column_names)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hzythx5uyePw"
      },
      "source": [
        "for i in range(len(aakash_df)):\n",
        "  temp = max(aakash_df['hate'].iloc[i],aakash_df['offen'].iloc[i],aakash_df['none'].iloc[i],aakash_df['prof'].iloc[i])\n",
        "  id_temp = aakash_df['id'].iloc[i]\n",
        "  if temp == aakash_df['hate'].iloc[i]:\n",
        "    aakash_result_df.loc[i] = [id_temp,'HATE']\n",
        "  elif temp == aakash_df['offen'].iloc[i]:\n",
        "    aakash_result_df.loc[i] = [id_temp,'OFFN']\n",
        "  elif temp == aakash_df['prof'].iloc[i]:\n",
        "    aakash_result_df.loc[i] = [id_temp,'PRFN']\n",
        "  elif temp == aakash_df['none'].iloc[i]:\n",
        "    aakash_result_df.loc[i] = [id_temp,'NONE']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82iJtyf1g4Tn"
      },
      "source": [
        "aakash_result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHbP5RiShGBq"
      },
      "source": [
        "values = (aakash_test['task_2'] == aakash_result_df['label']).value_counts()\n",
        "values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGPXd3-4h4Vo"
      },
      "source": [
        "accuracy = values[1]/(values[0]+values[1])\n",
        "accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gezx-DgkYWIi"
      },
      "source": [
        "#Submission file generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXrxquGAYZWa"
      },
      "source": [
        "submission_ids = test_hasoc_2._id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzNEIMq9maUC"
      },
      "source": [
        "proc_subm_df = df_preprocessing(test_hasoc_2,feature_col[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Id9lOYvEpNix"
      },
      "source": [
        "subm_data = data_tokenization(proc_subm_df,feature_col[0],max_len,tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4GqDHiLpdrS"
      },
      "source": [
        "submission_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX-B8ibppjWg"
      },
      "source": [
        "preds= bert.predict(subm_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPX57rWfp0pH"
      },
      "source": [
        "submiss_df= pd.DataFrame(preds, columns= target_col)\n",
        "submiss_df['id']=list(submission_ids)\n",
        "submiss_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E-bOXOGqfid"
      },
      "source": [
        "submiss_df = submiss_df[['id']+target_col.tolist()]\n",
        "submiss_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2WOFR6eqsvQ"
      },
      "source": [
        "column_names = [\"id\", \"label\"]\n",
        "submission_df = pd.DataFrame(columns = column_names)\n",
        "submission_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBNfN-a7qxWm"
      },
      "source": [
        "for i in range(len(submiss_df)):\n",
        "  temp = max(submiss_df['hate'].iloc[i],submiss_df['offen'].iloc[i],submiss_df['none'].iloc[i],submiss_df['prof'].iloc[i])\n",
        "  id_temp = submiss_df['id'].iloc[i]\n",
        "  if temp == submiss_df['hate'].iloc[i]:\n",
        "    submission_df.loc[i] = [id_temp,'HATE']\n",
        "  elif temp == submiss_df['offen'].iloc[i]:\n",
        "    submission_df.loc[i] = [id_temp,'OFFN']\n",
        "  elif temp == submiss_df['prof'].iloc[i]:\n",
        "    submission_df.loc[i] = [id_temp,'PRFN']\n",
        "  elif temp == submiss_df['none'].iloc[i]:\n",
        "    submission_df.loc[i] = [id_temp,'NONE']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYzvIFfdqzpV"
      },
      "source": [
        "submission_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV8Vwtz-rIbG"
      },
      "source": [
        "submission_df.to_csv('drive/MyDrive/Submissions/submission4_bert_hasoc_t2.csv', index = False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}